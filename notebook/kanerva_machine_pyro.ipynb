{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "from utils.custom_mlp import MLP, Exp\n",
    "from utils.mnist_cached import MNISTCached, mkdir_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "pyro.set_rng_seed(0)\n",
    "pyro.clear_param_store()\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for loading and batching MNIST dataset\n",
    "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
    "    root = './data'\n",
    "    download = True\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set = dset.MNIST(root=root, train=True, transform=trans,\n",
    "                           download=download)\n",
    "    test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "        batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CustomLinear(last_layer_size, layer_size, use_cuda):\n",
    "    # get our nn layer module (in this case nn.Linear by default)\n",
    "    cur_linear_layer = nn.Linear(last_layer_size, layer_size)\n",
    "    # for numerical stability -- initialize the layer properly\n",
    "    cur_linear_layer.weight.data.normal_(0, 0.001)\n",
    "    cur_linear_layer.bias.data.normal_(0, 0.001)\n",
    "    # use GPUs to share data during training (if available)\n",
    "    if use_cuda:\n",
    "        cur_linear_layer = nn.DataParallel(cur_linear_layer)\n",
    "    return cur_linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, n_address, n_memory_vec, n_address_vec, m_value = 1.0):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self.y_dim = n_memory_vec\n",
    "        self.z_dim = n_memory_vec\n",
    "        self.n_address = n_address\n",
    "        self.n_memory_vec = n_memory_vec\n",
    "        self.n_address_vec = n_address_vec\n",
    "        self.m_value = 1.0\n",
    "        self.R = torch.rand(self.n_address, self.n_memory_vec)\n",
    "        self.U = torch.eye(self.n_address)\n",
    "        self.V = torch.eye(self.n_memory_vec)\n",
    "        self.M = None\n",
    "        \n",
    "        self.A = nn.Parameter(torch.eye(self.n_address_vec, self.n_address))\n",
    "        self.fc_y1 = nn.Linear(self.y_dim, self.n_address_vec//2)\n",
    "        self.fc_y2 = nn.Linear(self.n_address_vec//2, self.n_address_vec)\n",
    "        \n",
    "    \n",
    "    def forward(self, _input, batch_size):\n",
    "        _y = self.fc_y1(_input)\n",
    "        bt = self.fc_y2(_y+0.08*torch.randn(_y.size()))\n",
    "        wt = self.A(bt)\n",
    "        return wt.view(batch_size, 1, -1)\n",
    "#         self.M = MultivariateNormal(self.R.view(-1, self.n_address*self.n_memory_vec), covariance_matrix=kronecker_product(self.V, self.U))\n",
    "    \n",
    "    \n",
    "    def write(self, y_list, Z, v_sigma):\n",
    "        W = None\n",
    "        len_t = len(y_list)\n",
    "        for _y in y_list:\n",
    "            _y = self.fc_y1(_y)\n",
    "            bt = self.fc_y2(_y)\n",
    "            wt = self.A(bt)\n",
    "            if W is None:\n",
    "                W = wt.view(1, self.n_address)\n",
    "            else:\n",
    "                W = torch.cat((W, wt), 0)\n",
    "        dd = Z - torch.matmul(W, self.R)\n",
    "        sigma_c  = torch.matmul(W, self.U)\n",
    "        sigma_gusai = torch.eye(len_t)\n",
    "        sigma_z = torch.matmul(torch.matmul(W, self.U), torch.t(W)) + sigma_gusai * (v_sigma*v_sigma)\n",
    "        R = self.R + torch.matmul(torch.matmul(torch.t(sigma_c),torch.inverse(sigma_z)), dd)\n",
    "        U = self.U - torch.matmul(torch.matmul(torch.t(sigma_c),torch.inverse(sigma_z)), sigma_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder1(nn.Module):\n",
    "    '''\n",
    "    return q(y|x) and q(z|x)\n",
    "    '''\n",
    "    def __init__(self, D_in, H, C):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, 2*C)\n",
    "        self.linear3y = torch.nn.Linear(2*C, C)\n",
    "        self.linear3z = torch.nn.Linear(2*C, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        y = F.relu(self.linear3y(x))\n",
    "        z = F.relu(self.linear3z(x))\n",
    "        return y, z\n",
    "\n",
    "\n",
    "class Encoder2(nn.Module):\n",
    "    '''\n",
    "    return q(z|x, y, M) from 2 C-size vector(q(z'|x) and p(z|y, M))\n",
    "    '''\n",
    "    def __init__(self, C, H):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(2*C, H)\n",
    "        self.linear2 = torch.nn.Linear(H, C)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        z = F.relu(self.linear2(x))\n",
    "        return z\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    return q(x|z)\n",
    "    '''\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return self.sigmoid(self.linear2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KanervaMachine(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_address=40,\n",
    "        n_memory_vec=50,\n",
    "        n_address_vec=25,\n",
    "        n_input=784,\n",
    "        n_hidden=100,\n",
    "        batch_size=10,\n",
    "        aux_loss_multiplier=None,\n",
    "        use_cuda=False\n",
    "    ):\n",
    "        super(KanervaMachine, self).__init__()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.n_address = n_address\n",
    "        self.n_memory_vec = n_memory_vec\n",
    "        self.n_address_vec = n_address_vec\n",
    "        self.n_latent_vec = n_memory_vec # the same size as one of the memory-cols\n",
    "        self.batch_size = batch_size\n",
    "        self.aux_loss_multiplier = aux_loss_multiplier\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.A = CustomLinear(self.n_address_vec, self.n_address, use_cuda) ## Pointer Matrix\n",
    "        self.R = None ##  (n_address × n_memory_vec) matrix as the mean of M\n",
    "        self.U = None ## (n_address × n_address matrix) that provides the covariance between rows of M\n",
    "        self.V = None ## (n_memory_vec × n_memory_vec) matrix that provides the covariance between cols of M\n",
    "        self.encoder1 = None\n",
    "        self.encoder2 = None\n",
    "        self.mlp_memory = None\n",
    "        self.decoder = None\n",
    "        self.allow_broadcast = False\n",
    "        self.setup_networks()\n",
    "    \n",
    "    def setup_networks(self):\n",
    "        self.encoder1 = MLP(\n",
    "            [self.n_input] + [self.n_hidden, ] + [[self.n_latent_vec, self.n_latent_vec, self.n_latent_vec, self.n_latent_vec]],\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=[None, None, Exp, Exp],\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda\n",
    "        )\n",
    "        self.encoder2 = MLP(\n",
    "            [self.n_latent_vec + self.n_latent_vec] + [self.n_hidden, ] + [[self.n_latent_vec, self.n_latent_vec]],\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=[None, Exp],\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda\n",
    "        )\n",
    "        self.decoder = MLP(\n",
    "            [self.n_latent_vec] + [self.n_hidden, ] + [self.n_input],\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Sigmoid,\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda\n",
    "        )\n",
    "        self.mlp_memory = MLP(\n",
    "            [self.n_latent_vec] + [self.n_address_vec//2, ] + [self.n_address_vec],\n",
    "            activation=nn.Softplus,\n",
    "            output_activation=nn.Softplus,\n",
    "            allow_broadcast=self.allow_broadcast,\n",
    "            use_cuda=self.use_cuda\n",
    "        )\n",
    "        self.R = torch.zeros(self.n_address, self.n_memory_vec, requires_grad=True)\n",
    "        self.U = torch.eye(self.n_address, requires_grad=True)\n",
    "        self.V = torch.eye(self.n_memory_vec, requires_grad=True)\n",
    "        \n",
    "    def model(self, xs):\n",
    "        \"\"\"\n",
    "        The model corresponds to the following generative process:\n",
    "        p(z) = normal(0,I)              # handwriting style (latent)\n",
    "        p(y|x) = categorical(I/10.)     # which digit (semi-supervised)\n",
    "        p(x|y,z) = bernoulli(loc(y,z))   # an image\n",
    "        loc is given by a neural network  `decoder`\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "#         print(\"model\")\n",
    "        pyro.module(\"kanerva_machine\", self)\n",
    "        \n",
    "        batch_size = xs.size(0)\n",
    "        with pyro.iarange(\"data\", xs.size(0)):\n",
    "            \n",
    "            # sample y, the latent vector as the key of the memory, from the constant prior distribution\n",
    "            y_prior_loc = xs.new_zeros([batch_size, self.n_latent_vec])\n",
    "            y_prior_scale = xs.new_ones([batch_size, self.n_latent_vec])\n",
    "            ys = pyro.sample(\"y\", dist.Normal(y_prior_loc, y_prior_scale).independent(1))\n",
    "            \n",
    "            # sample z, the latent vector to generate an image, from the memory with \"y\".\n",
    "            b = self.mlp_memory.forward(ys)\n",
    "            w = self.A(b)\n",
    "            z_loc = torch.matmul(w, self.R) # (batch_size, n_memory_vec)\n",
    "            z_scale = torch.ones(batch_size, self.n_memory_vec)\n",
    "            zs = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "            \n",
    "            # sample x, the target images, with \"z\".\n",
    "            x_loc = self.decoder.forward(zs)\n",
    "            pyro.sample(\"x\", dist.Bernoulli(x_loc).independent(1), obs=xs)\n",
    "            \n",
    "            return x_loc\n",
    "    \n",
    "    def guide(self, xs, len_t=30):\n",
    "        \"\"\"\n",
    "        The guide corresponds to the following:\n",
    "        q(y|x) = categorical(alpha(x))              # infer digit from an image\n",
    "        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer handwriting style from an image and the digit\n",
    "        loc, scale are given by a neural network `encoder_z`\n",
    "        alpha is given by a neural network `encoder_y`\n",
    "        :param xs: a batch of scaled vectors of pixels from an image\n",
    "        :param ys: (optional) a batch of the class labels i.e.\n",
    "                   the digit corresponding to the image(s)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Writing Phase:\n",
    "        #  sample y, z from q(y|x) and q(z|x)\n",
    "#         print(\"guide\")\n",
    "        with pyro.iarange(\"data\", xs.size(0)):\n",
    "            y_loc_w, z_loc_w, y_scale_w, z_scale_w = self.encoder1.forward(xs)\n",
    "            ys = pyro.sample(\"y\", dist.Normal(y_loc_w, y_scale_w).independent(1))\n",
    "            b = self.mlp_memory.forward(ys)\n",
    "            w = self.A(b)\n",
    "            self.write_inference(w, z_scale_w)\n",
    "        \n",
    "        #Reading Phase:\n",
    "        # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "            batch_size = xs.size(0)\n",
    "            b = self.mlp_memory.forward(ys)\n",
    "            w = self.A(b)\n",
    "            pre_z_loc = torch.matmul(w, self.R) # (batch_size, n_memory_vec)\n",
    "            z_loc, z_scale = self.encoder2([z_loc_w, pre_z_loc])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).independent(1))\n",
    "    \n",
    "    def write_inference(self, W, Z, v_sigma=1):\n",
    "        '''\n",
    "        Updating the Memory.\n",
    "        '''\n",
    "        dd = Z - torch.matmul(W, self.R)\n",
    "        sigma_c  = torch.matmul(W, self.U)\n",
    "        sigma_gusai = torch.eye(W.size(0))\n",
    "        sigma_z = torch.matmul(torch.matmul(W, self.U), torch.t(W)) + sigma_gusai * (v_sigma*v_sigma)\n",
    "        self.R = self.R + torch.matmul(torch.matmul(torch.t(sigma_c),torch.inverse(sigma_z)), dd)\n",
    "        self.U = self.U - torch.matmul(torch.matmul(torch.t(sigma_c),torch.inverse(sigma_z)), sigma_c)\n",
    "    \n",
    "    def init_memory(self):\n",
    "        self.R = torch.zeros(self.n_address, self.n_memory_vec, requires_grad=True)\n",
    "        self.U = torch.eye(self.n_address, requires_grad=True)\n",
    "        self.V = torch.eye(self.n_memory_vec, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for _, (x, _) in enumerate(train_loader):\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        b_size = x.size(0)\n",
    "        i_size = x.size(-1)*x.size(-1)\n",
    "        epoch_loss += svi.step(x.view(b_size, i_size))\n",
    "        print(epoch_loss)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for i, (x, _) in enumerate(test_loader):\n",
    "#         km.init_memory()\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        b_size = x.size(0)\n",
    "        i_size = x.size(-1)*x.size(-1)\n",
    "        test_loss += svi.evaluate_loss(x.view(b_size, i_size))\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KanervaMachine()\n",
    "\n",
    "adam_params = {\"lr\": 0.00042, \"betas\": (0.9, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "svi = SVI(km.model, km.guide, optimizer, loss=TraceEnum_ELBO(max_iarange_nesting=1), num_particles=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "NUM_EPOCHS = 100\n",
    "TEST_FREQUENCY = 5\n",
    "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152288.515625\n",
      "304689.921875\n",
      "457300.5\n",
      "609978.0\n",
      "762715.0625\n",
      "915330.28125\n",
      "1067963.921875\n",
      "1221261.375\n",
      "1374337.65625\n",
      "1527460.453125\n",
      "1680886.84375\n",
      "1834427.296875\n",
      "1987942.171875\n",
      "2141569.703125\n",
      "2295400.359375\n",
      "2449215.78125\n",
      "2602829.984375\n",
      "2756576.828125\n",
      "2910191.671875\n",
      "3064162.21875\n",
      "3217843.546875\n",
      "3371966.640625\n",
      "3525916.25\n",
      "3679610.265625\n",
      "3833900.703125\n",
      "3987880.390625\n",
      "4142318.796875\n",
      "4296252.171875\n",
      "4450515.859375\n",
      "4604649.03125\n",
      "4758846.15625\n",
      "4912894.03125\n",
      "5067381.3125\n",
      "5221869.03125\n",
      "5376376.109375\n",
      "5531120.15625\n",
      "5685947.0625\n",
      "5840700.78125\n",
      "5995407.84375\n",
      "6150243.53125\n",
      "6305127.71875\n",
      "6459654.25\n",
      "6614178.71875\n",
      "6768865.4375\n",
      "6923582.59375\n",
      "7078026.796875\n",
      "7232596.5625\n",
      "7387488.421875\n",
      "7542311.1875\n",
      "7697361.390625\n",
      "7852392.421875\n",
      "8007134.6875\n",
      "8162143.75\n",
      "8317054.265625\n",
      "8471656.796875\n",
      "8626795.84375\n",
      "8781761.671875\n",
      "8936864.765625\n",
      "9091619.15625\n",
      "9246555.390625\n",
      "9401475.078125\n",
      "9556625.03125\n",
      "9711718.0625\n",
      "9866592.734375\n",
      "10021450.484375\n",
      "10176413.5\n",
      "10331313.46875\n",
      "10486548.640625\n",
      "10641249.25\n",
      "10796491.234375\n",
      "10951833.4375\n",
      "11106647.40625\n",
      "11261823.984375\n",
      "11417005.15625\n",
      "11572258.921875\n",
      "11727312.171875\n",
      "11882605.0625\n",
      "12037941.421875\n",
      "12192920.390625\n",
      "12348324.9375\n",
      "12503459.40625\n",
      "12658472.890625\n",
      "12813702.40625\n",
      "12968953.546875\n",
      "13124441.3125\n",
      "13279463.640625\n",
      "13434500.171875\n",
      "13589825.578125\n",
      "13745252.78125\n",
      "13900396.140625\n",
      "14055707.265625\n",
      "14210800.328125\n",
      "14366074.4375\n",
      "14521630.640625\n",
      "14676794.59375\n",
      "14831793.515625\n",
      "14987100.8125\n",
      "15142423.03125\n",
      "15297866.984375\n",
      "15453534.84375\n",
      "15608702.96875\n",
      "15764476.46875\n",
      "15919828.609375\n",
      "16075345.84375\n",
      "16230592.84375\n",
      "16386171.890625\n",
      "16541275.453125\n",
      "16697062.78125\n",
      "16852597.125\n",
      "17008224.65625\n",
      "17163342.65625\n",
      "17318784.390625\n",
      "17474009.671875\n",
      "17629893.09375\n",
      "17785658.640625\n",
      "17941313.234375\n",
      "18096837.078125\n",
      "18252346.859375\n",
      "18408045.484375\n",
      "18563559.921875\n",
      "18719211.03125\n",
      "18874895.1875\n",
      "19030397.390625\n",
      "19186034.671875\n",
      "19341238.765625\n",
      "19497013.75\n",
      "19652867.96875\n",
      "19808364.0625\n",
      "19963939.90625\n",
      "20119700.5625\n",
      "20275378.0\n",
      "20430962.109375\n",
      "20586529.984375\n",
      "20742198.765625\n",
      "20897630.703125\n",
      "21053491.03125\n",
      "21208766.3125\n",
      "21364079.6875\n",
      "21519648.6875\n",
      "21675307.96875\n",
      "21830970.578125\n",
      "21986464.234375\n",
      "22142004.609375\n",
      "22297245.234375\n",
      "22452871.625\n",
      "22608479.046875\n",
      "22763990.484375\n",
      "22919704.8125\n",
      "23075104.65625\n",
      "23230694.15625\n",
      "23386366.90625\n",
      "23541890.984375\n",
      "23697423.5625\n",
      "23852907.734375\n",
      "24008750.40625\n",
      "24164298.578125\n",
      "24319717.046875\n",
      "24475519.21875\n",
      "24631148.5625\n",
      "24786737.5\n",
      "24942378.21875\n",
      "25097946.46875\n",
      "25253432.046875\n",
      "25408954.328125\n",
      "25564699.078125\n",
      "25720232.109375\n",
      "25876046.265625\n",
      "26031340.0\n",
      "26187193.671875\n",
      "26342698.3125\n",
      "26498716.546875\n",
      "26654116.84375\n",
      "26809733.625\n",
      "26965428.5\n",
      "27121104.5625\n",
      "27277138.40625\n",
      "27433581.90625\n",
      "27589272.984375\n",
      "27744893.578125\n",
      "27900611.578125\n",
      "28056035.375\n",
      "28211947.578125\n",
      "28367643.9375\n",
      "28523089.1875\n",
      "28678704.703125\n",
      "28834209.703125\n",
      "28990225.890625\n",
      "29145974.765625\n",
      "29301731.546875\n",
      "29457462.296875\n",
      "29613062.796875\n",
      "29768838.90625\n",
      "29924729.34375\n",
      "30080130.3125\n",
      "30235778.640625\n",
      "30391664.15625\n",
      "30547332.703125\n",
      "30702581.421875\n",
      "30858188.703125\n",
      "31014025.171875\n",
      "31169801.640625\n",
      "31325703.875\n",
      "31481395.890625\n",
      "31637097.390625\n",
      "31792850.75\n",
      "31948813.40625\n",
      "32104582.796875\n",
      "32260234.59375\n",
      "32416497.5\n",
      "32572269.171875\n",
      "32728339.171875\n",
      "32884016.859375\n",
      "33040004.640625\n",
      "33195636.453125\n",
      "33351307.0\n",
      "33506862.75\n",
      "33662442.125\n",
      "33817932.96875\n",
      "33973535.109375\n",
      "34129366.421875\n",
      "34284764.75\n",
      "34440469.984375\n",
      "34596295.453125\n",
      "34752136.53125\n",
      "34908154.8125\n",
      "35064021.875\n",
      "35219619.625\n",
      "35375227.8125\n",
      "35531102.03125\n",
      "35686742.921875\n",
      "35842528.921875\n",
      "35998527.15625\n",
      "36154227.3125\n",
      "36309947.109375\n",
      "36368274.0078125\n",
      "[epoch 000]  average training loss: 606.1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pyro/infer/traceenum_elbo.py:103: UserWarning: TraceEnum_ELBO found no sample sites configured for enumeration. If you want to enumerate sites, you need to @config_enumerate or set infer={\"enumerate\": \"sequential\"} or infer={\"enumerate\": \"parallel\"}? If you do not want to enumerate, consider using Trace_ELBO instead.\n",
      "  warnings.warn('TraceEnum_ELBO found no sample sites configured for enumeration. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000] average test loss: 608.8538\n",
      "155875.125\n",
      "311612.921875\n",
      "467689.40625\n",
      "623170.015625\n",
      "779243.6875\n",
      "935361.421875\n",
      "1091272.328125\n",
      "1247361.9375\n",
      "1403190.109375\n",
      "1559162.25\n",
      "1714986.34375\n",
      "1870671.15625\n",
      "2026426.65625\n",
      "2182581.765625\n",
      "2338710.03125\n",
      "2494271.578125\n",
      "2650099.5\n",
      "2806052.875\n",
      "2962397.265625\n",
      "3118352.53125\n",
      "3273977.734375\n",
      "3429894.375\n",
      "3585761.21875\n",
      "3742060.203125\n",
      "3897708.578125\n",
      "4053432.265625\n",
      "4209310.625\n",
      "4365051.484375\n",
      "4520900.671875\n",
      "4676783.40625\n",
      "4832720.328125\n",
      "4988806.640625\n",
      "5144515.28125\n",
      "5300195.34375\n",
      "5455950.21875\n",
      "5611767.6875\n",
      "5768328.9375\n",
      "5924262.578125\n",
      "6079902.921875\n",
      "6235890.6875\n",
      "6391976.78125\n",
      "6547777.46875\n",
      "6703909.65625\n",
      "6859526.125\n",
      "7015469.1875\n",
      "7171541.96875\n",
      "7327699.53125\n",
      "7483488.46875\n",
      "7639425.859375\n",
      "7795241.25\n",
      "7951086.203125\n",
      "8107283.609375\n",
      "8262937.359375\n",
      "8418594.765625\n",
      "8574298.6875\n",
      "8730230.78125\n",
      "8886235.453125\n",
      "9042156.75\n",
      "9198353.78125\n",
      "9354502.5\n",
      "9510604.140625\n",
      "9666648.609375\n",
      "9822533.921875\n",
      "9978681.09375\n",
      "10134741.859375\n",
      "10290820.78125\n",
      "10446864.375\n",
      "10603037.640625\n",
      "10758915.1875\n",
      "10914949.109375\n",
      "11070571.03125\n",
      "11226797.140625\n",
      "11382627.796875\n",
      "11538590.671875\n",
      "11694307.828125\n",
      "11850063.921875\n",
      "12006231.546875\n",
      "12162230.265625\n",
      "12317945.8125\n",
      "12473674.59375\n",
      "12629485.109375\n",
      "12785449.546875\n",
      "12941300.109375\n",
      "13097066.375\n",
      "13252741.9375\n",
      "13408877.546875\n",
      "13564580.625\n",
      "13720373.53125\n",
      "13876259.765625\n",
      "14031893.6875\n",
      "14187969.46875\n",
      "14343668.609375\n",
      "14499929.53125\n",
      "14655720.453125\n",
      "14811891.640625\n",
      "14967676.375\n",
      "15123467.296875\n",
      "15279443.921875\n",
      "15435623.625\n",
      "15591878.609375\n",
      "15747455.03125\n",
      "15903326.140625\n",
      "16058909.1875\n",
      "16214788.03125\n",
      "16370708.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-38:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-65fbf09bbfdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtotal_epoch_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_elbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotal_epoch_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[epoch %03d]  average training loss: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epoch_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-57f8fe183fc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(svi, train_loader, use_cuda)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mb_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mi_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/pyro/infer/traceenum_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melbo_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mloss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo_particle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/YumaKajihara/.pyenv/versions/anaconda3-2.5.0/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??SVI.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
